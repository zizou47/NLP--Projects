{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d077390",
   "metadata": {},
   "source": [
    "### Text Procssing 1\n",
    "* Tokenization >> lemmatization or stemming >> Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac5b177",
   "metadata": {},
   "source": [
    "1- tokenization is splitting text into words (word tokenization) or sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718dd39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The cats are running faster than the dogs in the park!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "702cf336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens : ['the', 'cats', 'are', 'running', 'faster', 'than', 'the', 'dogs', 'in', 'the', 'park', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "print(\"Tokens :\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95c61d",
   "metadata": {},
   "source": [
    "2- Stopwords is removing commun words like \"the\", \"is\", \"and\" and focusing on imprtant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae0d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt after stopwords ['cats', 'running', 'faster', 'dogs', 'park']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtred_tokens = []\n",
    "\n",
    "for word in tokens:\n",
    "    if word.isalnum() and word not in stop_words:\n",
    "        filtred_tokens.append(word)\n",
    "print('txt after stopwords', filtred_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd79fa49",
   "metadata": {},
   "source": [
    "3- Lemmatization is reducing the words in thier base ex : running >> run\n",
    "\n",
    "Use lemmatization when the understanding of the context and accurate reductions are more important. Use stemming when speed is more crucial and incorrect or non-dictionary words are acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f126773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt after lemmatazation ['cat', 'run', 'faster', 'dog', 'park']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word, pos='v') for word in filtred_tokens]\n",
    "\n",
    "print('txt after lemmatazation', lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84da8f",
   "metadata": {},
   "source": [
    "## Text Preprocessing 2 \n",
    " * BoW, TF-IDF, Unigrams, Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c25f3a",
   "metadata": {},
   "source": [
    "1 - Unigrams: Single words as features (e.g., \"cat\", \"run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283c45f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "documents = [\n",
    "    \"cat run fast park\",\n",
    "    \"dog chase cat park\",\n",
    "    \"cat dog play together\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b32cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Vocabulary: ['cat' 'chase' 'dog' 'fast' 'park' 'play' 'run' 'together']\n",
      "BoW matrix:\n",
      "   (0, 0)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "bow_vectorizer = CountVectorizer(ngram_range=(1,1))  \n",
    "bow_matrix = bow_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"BoW Vocabulary:\", bow_vectorizer.get_feature_names_out())\n",
    "print(\"BoW matrix:\\n\", bow_matrix)\n",
    "\n",
    "#  (numbre of doc, order of the word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737f586",
   "metadata": {},
   "source": [
    "2. Bigrams is Pairs of consecutive words (e.g., \"black cat\", \"run fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae7cbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Vocabulary: ['cat' 'chase' 'dog' 'fast' 'park' 'play' 'run' 'together']\n",
      "BoW Matrix:\n",
      " [[1 0 0 1 1 0 1 0]\n",
      " [1 1 1 0 1 0 0 0]\n",
      " [1 0 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "bow_matrix = bow_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"BoW Vocabulary:\", bow_vectorizer.get_feature_names_out())\n",
    "print(\"BoW Matrix:\\n\", bow_matrix.toarray())\n",
    "\n",
    "# matrix of vocabulary as features, documents as row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e28a1",
   "metadata": {},
   "source": [
    "3- Bag of Words (BoW): Represents text as a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f21dcda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unigram+Bigram Vocab: ['cat' 'cat dog' 'cat park' 'cat run' 'chase' 'chase cat' 'dog'\n",
      " 'dog chase' 'dog play' 'fast' 'fast park' 'park' 'play' 'play together'\n",
      " 'run' 'run fast' 'together']\n"
     ]
    }
   ],
   "source": [
    "# Unigrams + Bigrams combined with BoW\n",
    "unigram_bigram = CountVectorizer(ngram_range=(1,2))\n",
    "print(\"\\nUnigram+Bigram Vocab:\", unigram_bigram.fit(documents).get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f29d8f",
   "metadata": {},
   "source": [
    "4- TF-IDF:  improves BoW by giving higher weight to words\n",
    "* Common words like \"cat\" (appears in all 3 docs) get low weight.\n",
    "* Rare words like \"fast\" or \"chase\" (appear in only 1 doc) get high weight.\n",
    "\n",
    "* TF = number of reputation of words in sentences / number of words in the sentence  ;  IDF = log10(N/n)  ;  N = total number of documents(sentences)  ;  n = ns containins the word\n",
    "\n",
    "TF -IDF = TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95123b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_vecabulary ['cat' 'cat dog' 'cat park' 'cat run' 'chase' 'chase cat' 'dog'\n",
      " 'dog chase' 'dog play' 'fast' 'fast park' 'park' 'play' 'play together'\n",
      " 'run' 'run fast' 'together']\n",
      "tfidf_matrix [[0.2425937  0.         0.         0.41074684 0.         0.\n",
      "  0.         0.         0.         0.41074684 0.41074684 0.31238356\n",
      "  0.         0.         0.41074684 0.41074684 0.        ]\n",
      " [0.25171084 0.         0.4261835  0.         0.4261835  0.4261835\n",
      "  0.32412354 0.4261835  0.         0.         0.         0.32412354\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.2425937  0.41074684 0.         0.         0.         0.\n",
      "  0.31238356 0.         0.41074684 0.         0.         0.\n",
      "  0.41074684 0.41074684 0.         0.         0.41074684]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"tfidf_vecabulary\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"tfidf_matrix\", tfidf_matrix.toarray())\n",
    "\n",
    "# rare words now get +0.41 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4bfced",
   "metadata": {},
   "source": [
    "* Major issue with past technics is\n",
    " - sementic meaning ex (good = better) \n",
    " - sparser matrix (lot of zeros or decimal numbers in tf-idf) which lead to high dimentions\n",
    "\n",
    " which word2vec solve this issues by subtract similair words  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b929c",
   "metadata": {},
   "source": [
    "## Text Preprocessing 3 â†’ Word2Vec, Avg Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5855d5",
   "metadata": {},
   "source": [
    "Word embeddings has two types :\n",
    "\n",
    "* 1 count of frequency (BOW - TF-IDF - OHE)\n",
    "* 2   DL Trained model (Word2vec (1- CBOW), (2- SkipGrams))\n",
    " - so we use embedding layer to convert text into vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ef59a",
   "metadata": {},
   "source": [
    "suppose we have 2D vecotrs like  : man = [0.49, 0.98] , king = [0.75, 0.95], queen =[0.95, 0.74], woman = [-0.95, -0.88]\n",
    "\n",
    "king - man + woman = queen\n",
    "\n",
    "calculating this using cosine similarity (also we can use ecludian distance)\n",
    "\n",
    "* Distance = 1 - cosine similarity(cos teta) wich is the angel between the two points ex 45o  =  1-0.71 = 0.29\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83867a",
   "metadata": {},
   "source": [
    "expaining cbow + skipgram (for bi data)....\n",
    "there is word2vec goolgle-news-300 pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6fca2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
