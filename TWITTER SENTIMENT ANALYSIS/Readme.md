Emotion Classification on Twitter Data
A comparative study of three text classification approaches on the dair-ai/emotion dataset (Twitter messages labeled with 6 emotions: joy, sadness, anger, fear, love, surprise).
ğŸ“Š Results Summary





























ModelTest AccuracyValidation AccuracyNotesFCNN (TF-IDF + Dense layers)87.25%87.35%Fast baseline, mild overfittingBidirectional LSTM88.10%89.85%Strong sequential modelingDistilBERT (fine-tuned Transformer)92.40%93.85%Best performance â€“ clear winner with excellent generalization
ğŸ† Key Findings

Pretrained Transformers dramatically outperform classical and RNN-based methods.
Fine-tuning DistilBERT achieves near state-of-the-art (~93%) with only 3 epochs.
All models show good generalization (test â‰ˆ validation scores).
Class imbalance was handled effectively with class weights (critical for LSTM).

ğŸ“ Project Structure
text.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.txt
â”‚   â”œâ”€â”€ validation.txt
â”‚   â””â”€â”€ test.txt
â”œâ”€â”€ training.ipynb                # Full experiment notebook (local)
â”œâ”€â”€ Pretrained_model_GPU.ipynb    # DistilBERT fine-tuning on Colab GPU
â””â”€â”€ README.md
ğŸš€ How to Run
Option 1: Local (FCNN + BiLSTM)

Install dependencies:Bashpip install pandas nltk tensorflow scikit-learn
Run training.ipynb in Jupyter.

Option 2: DistilBERT (Recommended â€“ Best Results)

Open Pretrained_model_GPU.ipynb in Google Colab
Enable GPU: Runtime â†’ Change runtime type â†’ GPU
Upload train.txt and test.txt
Run all cells (~10 minutes total)

ğŸ”® Prediction Example (DistilBERT)
Pythonfrom transformers import pipeline

classifier = pipeline("text-classification", model="your_saved_distilbert_model")
classifier("I feel so happy today!")  # â†’ joy
ğŸ“ What I Learned

Start with simple baselines, but always try a Hugging Face pretrained model for text tasks.
Transformers win on accuracy and generalization with minimal tuning.
Proper evaluation (held-out validation) prevents test overfitting.

For any new text classification project: fine-tune a small pretrained Transformer first.
